{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "# specify GPU\n",
    "#device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\VS_NLP_2\\lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "c:\\Anaconda3\\envs\\VS_NLP_2\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "c:\\Anaconda3\\envs\\VS_NLP_2\\lib\\site-packages\\transformers\\generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "c:\\Anaconda3\\envs\\VS_NLP_2\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Źródło:\n",
    "https://www.kaggle.com/code/harshjain123/bert-for-everyone-tutorial-implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>z czym przychodzicie</td>\n",
       "      <td>pytanie</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tak wiecej kaszlal</td>\n",
       "      <td>objaw</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kaszlal na sucho</td>\n",
       "      <td>objaw</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mial mokry kaszel</td>\n",
       "      <td>objaw</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mial szczekajacy kaszel</td>\n",
       "      <td>objaw</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      text category  label\n",
       "0     z czym przychodzicie  pytanie      5\n",
       "1       tak wiecej kaszlal    objaw      4\n",
       "2         kaszlal na sucho    objaw      4\n",
       "3        mial mokry kaszel    objaw      4\n",
       "4  mial szczekajacy kaszel    objaw      4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('data/vocabularz_2410.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "4    0.300863\n",
       "5    0.281545\n",
       "3    0.241677\n",
       "2    0.067818\n",
       "0    0.064118\n",
       "1    0.043979\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check class distribution\n",
    "df['label'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_text, temp_text, train_pytanie, temp_pytanie,train_odpowiedz, temp_odpowiedz,  = train_test_split(df['text'], df['pytanie'],df['odpowiedz'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.8, \n",
    "                                                                    stratify=df['odpowiedz'])\n",
    "\n",
    "\n",
    "val_text, test_text, val_pytanie, test_pytanie, val_odpowiedz, test_odpowiedz = train_test_split(temp_text, temp_pytanie, temp_odpowiedz, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.1, \n",
    "                                                                stratify=temp_odpowiedz)\n",
    "\"\"\"\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['label'])\n",
    "\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Bert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "#bert = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "#tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\VS_NLP_2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert lists to tensors\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        \n",
    "        self.bert = bert \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,6)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        \n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "      \n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "      \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\VS_NLP_2\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [2.60397554 3.78444444 2.44683908 0.68891586 0.55436198 0.59255393]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "\n",
    "class_weights = compute_class_weight(class_weight = 'balanced', classes = np.unique(train_labels), y = train_labels)\n",
    "\n",
    "print(\"Class Weights:\",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# push to GPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "    \n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        \n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    " \n",
    "        sent_id, mask, labels = batch\n",
    "        \n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "      # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "    \n",
    "    print(\"\\nEvaluating...\")\n",
    "  \n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "        \n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            \n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "  Batch    50  of     54.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.797\n",
      "Validation Loss: 1.788\n",
      "\n",
      " Epoch 2 / 10\n",
      "  Batch    50  of     54.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.793\n",
      "Validation Loss: 1.788\n",
      "\n",
      " Epoch 3 / 10\n",
      "  Batch    50  of     54.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.793\n",
      "Validation Loss: 1.787\n",
      "\n",
      " Epoch 4 / 10\n",
      "  Batch    50  of     54.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.795\n",
      "Validation Loss: 1.787\n",
      "\n",
      " Epoch 5 / 10\n",
      "  Batch    50  of     54.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.791\n",
      "Validation Loss: 1.787\n",
      "\n",
      " Epoch 6 / 10\n",
      "  Batch    50  of     54.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.789\n",
      "Validation Loss: 1.786\n",
      "\n",
      " Epoch 7 / 10\n",
      "  Batch    50  of     54.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.786\n",
      "Validation Loss: 1.787\n",
      "\n",
      " Epoch 8 / 10\n",
      "  Batch    50  of     54.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.791\n",
      "Validation Loss: 1.786\n",
      "\n",
      " Epoch 9 / 10\n",
      "  Batch    50  of     54.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.788\n",
      "Validation Loss: 1.785\n",
      "\n",
      " Epoch 10 / 10\n",
      "  Batch    50  of     54.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 1.785\n",
      "Validation Loss: 1.786\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        #torch.save(model.state_dict(), 'FineTuneBertVocabularz6etykiet.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'FineTuneBertVocabularz6etykiet.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq.to(device), test_mask.to(device))\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        24\n",
      "           1       0.00      0.00      0.00        16\n",
      "           2       0.14      0.42      0.22        24\n",
      "           3       0.28      0.27      0.27        88\n",
      "           4       0.30      0.12      0.17       110\n",
      "           5       0.35      0.54      0.43       103\n",
      "\n",
      "    accuracy                           0.28       365\n",
      "   macro avg       0.18      0.23      0.18       365\n",
      "weighted avg       0.26      0.28      0.25       365\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\VS_NLP_2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Anaconda3\\envs\\VS_NLP_2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Anaconda3\\envs\\VS_NLP_2\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wynik dla zadanego tekstu : 5 a to jest :\n",
      "pytanie\n"
     ]
    }
   ],
   "source": [
    "example_text = \"poszliśmy do doktor Krztoń,\"\n",
    "bert_input = tokenizer(example_text,padding='max_length', max_length = 512, \n",
    "                       truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "input = bert_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "\n",
    "mask = bert_input['attention_mask'].to(device)\n",
    "result = model(input, mask)\n",
    "score = (result.argmax(dim=1))\n",
    "score = score.tolist()\n",
    "\n",
    "print(\"wynik dla zadanego tekstu :\", score[0], \"a to jest :\")\n",
    "\n",
    "if score[0]==0: \n",
    "    print('badanie')\n",
    "elif score[0]==1: \n",
    "    print('czestotliwosc')\n",
    "elif score[0]==2: \n",
    "    print('diagnostyka')\n",
    "elif score[0]==3: \n",
    "    print('lek')\n",
    "elif score[0]==4:\n",
    "    print('objaw')\n",
    "elif score[0]==5: \n",
    "    print('pytanie')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VS_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
